{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Another potential issue is that the model may not be optimized for performance, and there may be ways to improve the accuracy of the model while reducing the number of epochs needed for training. Performance tuning can be done by experimenting with different hyperparameters, such as the learning rate, batch size, and number of hidden layers and neurons. Additionally, other types of regularization techniques, such as dropout or early stopping, can also be tried.\n",
        "\n"
      ],
      "metadata": {
        "id": "blgYwK09zHIy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxoTjhHBzGZv"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib import pyplot\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "#before trianing the model split insto training sets 70/30\n",
        "n_train = 30\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "\n",
        "#define the model linear function Sequential, 500 nodes, \n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer=l2(0.001))) # removed kernel otherwise all codes remain the samee\n",
        "model.add(Dense(500, input_dim=2, activation='relu')) # one hidden layer with more node than is required to solve the problems, and longer training time. \n",
        "model.add(Dense(1, activation='sigmoid')) # output layer predict the class value 01\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # optimize using 'binary_crossentropy' function, effiecient 'adam' of gradient decent. \n",
        "\n",
        "history = model.fit(trainX, trainy, epochs=4000, validation_data=(testX, testy), verbose=0) # training model at 4000 epochs and fault default size of 32\n",
        "\n",
        "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "_, test_acc = model.evaluate(testX, testy, verbose=0) # using test dataset as validation model performance and report the results\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Cross-Entropy Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model it has better performance than in the training dataset 1.o test .91. Possble sign of overfitting. Cross-entropy loss show overfitting. \n",
        "\n",
        "Now lets work on MLP model by adding weight regularization to the hidden layer to reduce the overfitting of the model. Using L2 vector norm lambdia of 0.001. Done by adding kernel_regularizer. "
      ],
      "metadata": {
        "id": "Cm2guLla21pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prompt_toolkit.shortcuts.dialogs import Label\n",
        "from sklearn.datasets import make_moons\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from matplotlib import pyplot\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "n_train = 30\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "\n",
        "values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "all_train, all_test = list(), list()\n",
        "for param in values:\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer=l2(0.001))) # lambdia .001\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " \n",
        " #add\n",
        "history = model.fit(trainX, trainy, epochs=4000, validation_data=(testX, testy), verbose=0)\n",
        "\n",
        "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "print('Param: %f, Train: %.3f, Test: %.3f' % (param, train_acc, test_acc))\n",
        " \n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy', pad=-40)\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "id": "Uxdyflwd22EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No change in the accuracy, the learning curve plato indicating model no longer overfitting the dataset. "
      ],
      "metadata": {
        "id": "NSTWQG5tE4W2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from matplotlib import pyplot\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n",
        "\n",
        "n_train = 30\n",
        "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
        "trainy, testy = y[:n_train], y[n_train:]\n",
        "\n",
        "values = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6] #grid search \n",
        "all_train, all_test = list(), list()\n",
        "for param in values:\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(500, input_dim=2, activation='relu', kernel_regularizer=l2(param)))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\tmodel.fit(trainX, trainy, epochs=4000, verbose=0)\n",
        "\t_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
        "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
        "\tprint('Param: %f, Train: %.3f, Test: %.3f' % (param, train_acc, test_acc))\n",
        "\tall_train.append(train_acc)\n",
        "\tall_test.append(test_acc)\n",
        "\n",
        "pyplot.semilogx(values, all_train, label='train', marker='o')\n",
        "pyplot.semilogx(values, all_test, label='test', marker='o')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "1wwMDfIiFktA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The grid search regularization, test the value.\n",
        "In this result suggest that .01 or .001 is sufficient. \n",
        "Line is showing the increase in the test accuracy wider. Increase larger value\n",
        "drop in both train and test values. \n"
      ],
      "metadata": {
        "id": "lvIuxN40GHZa"
      }
    }
  ]
}